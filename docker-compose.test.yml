services:
  ollama:
    build: 
      context: .
      dockerfile: Dockerfile.ollama
    image: ollama/ollama
    container_name: ollama-test
    ports:
      - "11434:11434"
    volumes:
      # Requieren declaración en la sección final "volumes:"" Persisten incluso si eliminas el contenedor, pero no si eliminas explícitamente el volumen
      - ollama_data:/root/.ollama # se almacenan en un espacio gestionado por Docker, dentro del servicio (dice dónde montarlo)
    # Descomenta la siguiente sección si tienes una GPU NVIDIA y NVIDIA Container Toolkit instalado
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
  app:
    build: .
    container_name: containerai-test
    restart: unless-stopped # resilient against failures and automatically recover without manual intervention
    # ports:
    #   - "8000:8080" # Nginx se encarga
    # volumes:
    #   - ./app:/code/app # En test no edito no necesito actualizar en tiempo real como en dev
    depends_on:
      - ollama # nombre del servicio no del contenedor
    environment:
      - OLLAMA_HOST=http://ollama:11434
  nginx:
    image: nginx:alpine
    container_name: nginx-test
    restart: unless-stopped # resilient against failures and automatically recover without manual intervention
    ports:
      - "80:80"
    volumes:
      - ./nginx-test.conf:/etc/nginx/nginx.conf:ro

volumes:
  ollama_data: