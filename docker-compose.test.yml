services:
  # 1. El servicio del message broker (RabbitMQ)
  rabbitmq:
      image: rabbitmq:4.1.4-management-alpine # Imagen oficial con interfaz de gestión
      container_name: 'rabbitmq-test'
      ports:
        - "5672:5672"   # Puerto para que la aplicación se comunique con RabbitMQ
        - "15672:15672" # Puerto para acceder a la interfaz web de gestión de RabbitMQ
      # volumes:
      #   - ~/.docker-conf/rabbitmq/data/:/var/lib/rabbitmq/
      #   - ~/.docker-conf/rabbitmq/log/:/var/log/rabbitmq
      volumes:
        # Es más portable y manejable por Docker
        - rabbitmq_data:/var/lib/rabbitmq/
      networks:
        - app-network
  # 2. Servicio de Ollama
  ollama:
    build: 
      context: .
      dockerfile: Dockerfile.ollama
    # image: ollama/ollama # Es mejor usar la imagen que construyes en Dockerfile.ollama
    container_name: ollama-test
    ports:
      - "11434:11434"
    volumes:
      # Requieren declaración en la sección final "volumes:"" Persisten incluso si eliminas el contenedor, pero no si eliminas explícitamente el volumen
      - ollama_data:/root/.ollama # se almacenan en un espacio gestionado por Docker, dentro del servicio (dice dónde montarlo)
    networks:
      - app-network
    # Descomenta la siguiente sección si tienes una GPU NVIDIA y NVIDIA Container Toolkit instalado
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
  # 3. Servicio de la aplicación (API fastAPI)
  app:
    build: .
    container_name: containerai-test
    restart: unless-stopped # resilient against failures and automatically recover without manual intervention
    # ports:
    #   - "8000:8080" # Nginx se encarga
    # volumes:
    #   - ./app:/code/app # En test no edito no necesito actualizar en tiempo real como en dev
    depends_on:
      - ollama # nombre del servicio no del contenedor
      - rabbitmq # inicie RabbitMQ antes que la app
    environment:
      - OLLAMA_HOST=http://ollama:11434
    networks:
      - app-network
  # 4. El servicio del worker de Celery
  worker:
    build: . # Usa la misma imagen que la aplicación 'app'
    container_name: 'celery_worker-test'
    restart: unless-stopped
    command: celery -A app.celery.worker.celery_app worker --loglevel=info # módulo 'tasks' dentro del paquete 'app/celery/worker'
    # El código se debe tomar de la imagen construida, no del host.
    # Descomenta la siguiente línea solo para desarrollo:
    # volumes:
    #   - ./app:/code/app
    depends_on:
      - rabbitmq
      - app
      - ollama
    environment:
      - OLLAMA_HOST=http://ollama:11434
    networks:
      - app-network
  # 5. Servicio de redis
  redis:
    image: redis:7-alpine
    container_name: redis-cache-test
    restart: unless-stopped
    ports:
      - "6379:6379" # Expone el puerto 6379 del contenedor al puerto 6379 de tu máquina
    # Probar sistemas de caché que deben sobrevivir a un reinicio del servicio
    volumes:
      - redis_data:/data # verificar que Celery se guarda y persiste la data en la semana
    networks:
      - app-network
  # 6. Servicio de Nginx
  nginx:
    image: nginx:alpine
    container_name: nginx-test
    restart: unless-stopped # resilient against failures and automatically recover without manual intervention
    ports:
      - "80:80"
    volumes:
      - ./nginx-test.conf:/etc/nginx/nginx.conf:ro
    # --- depends_on CON CONDICIÓN DE SALUD ACTIVADO ---
    depends_on:
      - app
    networks:
      - app-network

# Define los volúmenes nombrados para persistencia de datos
volumes:
  ollama_data:
  rabbitmq_data:
  redis_data:

# Define la red compartida
networks:
  app-network:
    driver: bridge
